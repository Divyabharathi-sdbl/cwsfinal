# -*- coding: utf-8 -*-
"""cwsfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3cZz_OLokxEWV3DjpoDFK5EPtmbATe5
"""

import os
import json
import math
from typing import Tuple, Dict

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers

def generate_synthetic_hierarchy(n_series_per_leaf=4, n_leaves=4, seq_len=200, seasonal_period=24, noise_std=0.05, seed=42):
    np.random.seed(seed)
    timestamps = pd.date_range(start='2020-01-01', periods=seq_len, freq='H')
    rows = []
    n_groups = n_leaves
    for g in range(n_groups):
        for s in range(n_series_per_leaf):
            series_id = f'g{g}_s{s}'
            trend = 0.01 * np.arange(seq_len)
            seasonal = 2.0 * np.sin(2 * np.pi * np.arange(seq_len) / seasonal_period + np.random.randn()*0.1)
            group_effect = (g - n_groups/2) * 0.1
            noise = np.random.normal(scale=noise_std, size=seq_len)
            values = (10 + group_effect) + trend + seasonal + noise
            for t, v in enumerate(values):
                rows.append({'timestamp': timestamps[t], 'series_id': series_id, 'group': f'g{g}', 'value': float(v)})
    df = pd.DataFrame(rows)
    return df

def pivot_to_matrix(df: pd.DataFrame) -> Tuple[np.ndarray, list, pd.DatetimeIndex]:
    pivot = df.pivot(index='timestamp', columns='series_id', values='value')
    pivot = pivot.sort_index()
    series_ids = list(pivot.columns)
    return pivot.values, series_ids, pivot.index

def create_supervised(data: np.ndarray, input_len: int, output_len: int):
    T, n_series = data.shape
    samples = []
    targets = []
    for i in range(T - input_len - output_len + 1):
        x = data[i:i+input_len]
        y = data[i+input_len:i+input_len+output_len]
        samples.append(x)
        targets.append(y)
    X = np.stack(samples)
    Y = np.stack(targets)
    return X, Y

def build_model(input_len:int, n_series:int, hidden_units=64, num_layers=1, dropout=0.1, lr=1e-3):
    inputs = layers.Input(shape=(input_len, n_series), name='inputs')
    x = layers.TimeDistributed(layers.Dense(hidden_units))(inputs)
    for _ in range(num_layers):
        x = layers.Bidirectional(layers.LSTM(hidden_units, return_sequences=True))(x)
        x = layers.Dropout(dropout)(x)
    score = layers.TimeDistributed(layers.Dense(1))(x)
    score = layers.Flatten()(score)
    attn_weights = layers.Activation('softmax', name='attn_weights')(score)
    attn_weights_exp = layers.Reshape((1, -1))(attn_weights)
    x_permuted = layers.Permute((2,1))(x)
    context = layers.Dot(axes=[2,2])([attn_weights_exp, x_permuted])
    context = layers.Flatten()(context)
    out = layers.Dense(n_series, name='pred')(context)
    model = models.Model(inputs=inputs, outputs=out)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')
    return model

def bottom_up_reconcile(y_hat_leaf: np.ndarray, aggregation_matrix: np.ndarray) -> np.ndarray:
    return y_hat_leaf.dot(aggregation_matrix.T)

def wape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

def run_pipeline(input_len=48, output_len=1, hidden_units=64, num_layers=1, epochs=10, batch_size=32, search_space=None, random_search_trials=4, seed=123):
    df = generate_synthetic_hierarchy(n_series_per_leaf=3, n_leaves=5, seq_len=600)
    data_matrix, series_ids, timestamps = pivot_to_matrix(df)
    T, n_series = data_matrix.shape

    scaler = StandardScaler()
    data_norm = scaler.fit_transform(data_matrix)

    X, Y = create_supervised(data_norm, input_len=input_len, output_len=output_len)
    Y = Y[:,0,:]

    n = X.shape[0]
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)
    X_train, Y_train = X[:train_end], Y[:train_end]
    X_val, Y_val = X[train_end:val_end], Y[train_end:val_end]
    X_test, Y_test = X[val_end:], Y[val_end:]

    best = {'score': float('inf')}
    if search_space is None:
        search_space = {'hidden_units':[32,64], 'num_layers':[1,2], 'dropout':[0.1], 'lr':[1e-3]}

    rng = np.random.RandomState(seed)
    for t in range(random_search_trials):
        params = {k: rng.choice(v) for k,v in search_space.items()}
        print(f"Trial {t+1}/{random_search_trials} params={params}")
        tf.keras.backend.clear_session()
        model = build_model(input_len=input_len, n_series=n_series, hidden_units=params['hidden_units'], num_layers=params['num_layers'], dropout=params.get('dropout',0.1), lr=params.get('lr',1e-3))
        es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=0)
        val_pred = model.predict(X_val)
        val_rmse = math.sqrt(mean_squared_error(Y_val, val_pred))
        print(f"  val_rmse={val_rmse:.4f}")
        if val_rmse < best['score']:
            best.update({'score': val_rmse, 'params': params, 'model': model})
    print("Best params:", best['params'], "val_rmse=", best['score'])

    model = best['model']
    y_pred_test = model.predict(X_test)

    def inv_transform_batch(y_batch_norm):
        n_s = y_batch_norm.shape[0]
        stacked = np.vstack([y_batch_norm])
        inv = scaler.inverse_transform(stacked)
        return inv.reshape(n_s, -1)

    Y_test_inv = inv_transform_batch(Y_test)
    y_pred_inv = inv_transform_batch(y_pred_test)

    mae = mean_absolute_error(Y_test_inv, y_pred_inv)
    rmse = math.sqrt(mean_squared_error(Y_test_inv, y_pred_inv))
    wap = wape(Y_test_inv, y_pred_inv)
    print(f"Test MAE={mae:.4f}, RMSE={rmse:.4f}, WAPE={wap:.4f}")

    n_nodes = n_series
    aggregation_matrix = np.eye(n_series)
    reconciled = bottom_up_reconcile(y_pred_inv, aggregation_matrix)

    out_dir = 'artifact_out'
    os.makedirs(out_dir, exist_ok=True)
    model.save(os.path.join(out_dir, 'best_model.keras'))

    # Convert numpy int64 values in best['params'] to standard Python int for JSON serialization
    serializable_params = {k: int(v) if isinstance(v, np.int64) else v for k, v in best['params'].items()}

    with open(os.path.join(out_dir, 'metadata.json'), 'w') as f:
        json.dump({'params': serializable_params, 'test_metrics': {'mae': mae, 'rmse': rmse, 'wape': wap}}, f, indent=2)
    print('Artifacts saved to', out_dir)
    print('rms')
    print(rmse)
    print('mae')
    print(mae)
    print('wape')
    print(wap)
    print('reconciled')
    print(reconciled)

    return {
        'model': model,
        'scaler': scaler,
        'series_ids': series_ids,
        'metrics': {'mae': mae, 'rmse': rmse, 'wape': wap}
    }

if __name__ == '__main__':
    results = run_pipeline(epochs=6, random_search_trials=2)
    print('Done')